# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115v-UV5dP5_y3SteYT4SSq8JZ22exY-D
"""

!pip install openslide-bin
!pip install openslide-python openslide-bin

import openslide

# √énlocuie»ôte cu calea fi»ôierului tƒÉu WSI
wsi_path = "/content/wsi_input/11CO054-f6a7f764-2495-4a1d-89a2-e12f51.svs"

try:
    slide = openslide.OpenSlide(wsi_path)
    print("‚úÖ Fi»ôierul este compatibil cu OpenSlide.")

    # Informa»õii generale
    print(f"- Dimensiuni nivel 0: {slide.dimensions}")
    print(f"- NumƒÉr niveluri: {slide.level_count}")
    print(f"- Dimensiuni pe niveluri: {slide.level_dimensions}")

    # Rezolu»õia √Æn microni per pixel
    mpp_x = slide.properties.get("openslide.mpp-x")
    mpp_y = slide.properties.get("openslide.mpp-y")
    print(f"- Rezolu»õie: {mpp_x} x {mpp_y} microni/pixel")

    # Vendor info
    vendor = slide.properties.get("openslide.vendor")
    print(f"- Vendor: {vendor}")

    # Po»õi salva un thumbnail (ex: 512x512)
    thumb = slide.get_thumbnail((512, 512))
    thumb.save("/content/thumbnail.png")
    print("üñºÔ∏è Thumbnail salvat: /content/thumbnail.png")

    slide.close()

except openslide.OpenSlideUnsupportedFormatError:
    print("‚ùå Fi»ôierul NU este compatibil cu OpenSlide.")
except Exception as e:
    print(f"‚ùå Eroare: {e}")

import openslide
import os
import numpy as np
from PIL import Image
import cv2
from sklearn.cluster import KMeans
from tqdm import tqdm

# SetƒÉri
wsi_path = "/content/wsi_input/11CO054-f6a7f764-2495-4a1d-89a2-e12f51.svs"
patch_size = 224
output_dir = "/content/wsi_output"
os.makedirs(output_dir, exist_ok=True)

# Deschidere WSI »ôi generare thumbnail
slide = openslide.OpenSlide(wsi_path)
thumb = slide.get_thumbnail((1024, 1024))
thumb_np = np.array(thumb)

# KMeans pe thumbnail pentru segmentarea »õesutului (background = cel mai luminos cluster)
h, w, _ = thumb_np.shape
flat = thumb_np.reshape(-1, 3).astype(np.float32) / 255.0
feats = np.c_[flat.mean(1), flat.std(1)]
km = KMeans(n_clusters=2, random_state=0, n_init="auto").fit(feats)
labels = km.labels_.reshape(h, w)
bg_cluster = np.argmax([feats[km.labels_ == i, 0].mean() for i in range(2)])
mask = (labels != bg_cluster).astype(np.uint8) * 255

# Morfologie (curƒÉ»õare mascƒÉ)
kernel = np.ones((5, 5), np.uint8)
mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

# Mapare mascƒÉ la dimensiunile nivelului 0
w0, h0 = slide.dimensions
mask_resized = cv2.resize(mask, (w0, h0), interpolation=cv2.INTER_NEAREST)

# Extragem patchuri doar din zonele cu »õesut
count = 0
for y in tqdm(range(0, h0 - patch_size, patch_size)):
    for x in range(0, w0 - patch_size, patch_size):
        region_mask = mask_resized[y:y+patch_size, x:x+patch_size]
        if np.count_nonzero(region_mask) == 0:
            continue  # doar fundal

        patch = slide.read_region((x, y), 0, (patch_size, patch_size)).convert("RGB")
        patch.save(os.path.join(output_dir, f"patch_{count:05d}.png"))
        count += 1

print(f"‚úÖ Total patchuri extrase: {count}")
slide.close()

import os
import shutil
from tqdm import tqdm

src_dir = "/content/wsi_output"
dst_dir = "/content/patches_for_ssl"
os.makedirs(dst_dir, exist_ok=True)

# Copiere patchuri (dureazƒÉ c√¢teva minute pentru 50k imagini)
patches = sorted(os.listdir(src_dir))
for fname in tqdm(patches, desc="Copiere patchuri"):
    shutil.copy(os.path.join(src_dir, fname), os.path.join(dst_dir, fname))

print(f"‚úÖ {len(patches)} imagini copiate √Æn {dst_dir}")

# CURƒÇ»öARE RAM »ôi GPU
import gc
import torch
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# 1. Folder nou cu doar 5.000 imagini
import os, random, shutil
from tqdm import tqdm

src = "/content/patches_for_ssl"
dst = "/content/ssl_subset_5k"
os.makedirs(dst, exist_ok=True)

sampled = random.sample(os.listdir(src), 5000)
for f in tqdm(sampled, desc="Copiere subset 5k"):
    shutil.copy(os.path.join(src, f), os.path.join(dst, f))

# 1. Instalare
!pip install lightly --quiet

# 2. Importuri
import os, random, torch, numpy as np
from torch import nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms as T
import torchvision.models as models
from lightly.loss import NegativeCosineSimilarity
from lightly.models.modules.heads import BYOLProjectionHead
from tqdm import tqdm

# 3. Config
PATCH_DIR = "/content/ssl_subset_5k"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 16
EPOCHS = 3
IMAGE_SIZE = 224

# 4. Dataset
class PatchDataset(Dataset):
    def __init__(self, folder, transform):
        self.paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(".png")]
        self.transform = transform
    def __len__(self): return len(self.paths)
    def __getitem__(self, idx):
        img = Image.open(self.paths[idx]).convert("RGB")
        return self.transform(img), self.transform(img)

# 5. TransformƒÉri augmentate (BYOL)
transform = T.Compose([
    T.Resize(IMAGE_SIZE),
    T.RandomHorizontalFlip(),
    T.RandomGrayscale(p=0.2),
    T.ToTensor(),
    T.Normalize((0.5,), (0.5,))
])


dataset = PatchDataset(PATCH_DIR, transform)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

class BYOL(nn.Module):
    def __init__(self):
        super().__init__()

        def get_resnet_backbone():
            resnet = models.resnet50(weights=None)
            backbone = nn.Sequential(*list(resnet.children())[:-1], nn.Flatten())
            return backbone

        self.online_encoder = nn.Sequential(
            get_resnet_backbone(),
            BYOLProjectionHead(2048, 2048, 512)
        )

        self.target_encoder = nn.Sequential(
            get_resnet_backbone(),
            BYOLProjectionHead(2048, 2048, 512)
        )

        self.predictor = BYOLProjectionHead(512, 512, 512)
        self.criterion = NegativeCosineSimilarity()

        for param_q, param_k in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False


    def update_moving_average(self, beta=0.99):
        for param_q, param_k in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):
            param_k.data = param_k.data * beta + param_q.data * (1 - beta)

    def forward(self, x1, x2):
        z1 = self.online_encoder(x1)
        z2 = self.online_encoder(x2)
        p1 = self.predictor(z1)
        p2 = self.predictor(z2)

        with torch.no_grad():
            t1 = self.target_encoder(x1)
            t2 = self.target_encoder(x2)

        loss = 0.5 * (self.criterion(p1, t2) + self.criterion(p2, t1))
        return loss

# 7. Antrenare
model = BYOL().to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(EPOCHS):
    total_loss = 0
    for (x1, x2) in tqdm(loader, desc=f"Epoca {epoch+1}/{EPOCHS}"):
        x1, x2 = x1.to(DEVICE), x2.to(DEVICE)
        loss = model(x1, x2)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        model.update_moving_average()
        total_loss += loss.item()
    print(f"‚úÖ Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}")

# 8. Salvare encoder
torch.save(model.online_encoder[0].state_dict(), "/content/byol_backbone_resnet50_light.pth")
print("üéâ Backbone BYOL salvat.")